---
title: "Examine model outputs"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Examine model outputs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Describing the model output

## `tidy()`, `glance()` and `augment()`

These functions from `generics` will return the coefficients, models settings, and out of fold predictions (hat) and residuals (res) respectively for both the outcome model (g) and treatment model (m).

```{r run-rf}
library(tiDML)
df <- diamonds_sample(n=10000)

fit_rf <- dml_rf(
  data = df,
  y = "price",
  d = "is_rated_ideal",
  x = c("carat", "depth", "table", "x", "y", "z"),
)
```

1. tidy() returns the coefficients and standard errors
```{r tidy}
tidy(fit_rf)
```
2. glance() returns the model settings
```{r glance}
glance(fit_rf)
```
3. augment() returns the original data, predicted values, and residuals
```{r augment}
augment(fit_rf)
```

# Inspecting the workflow

The workflow is defined using the `workflows` package. This combines pre-processing (`recipes`) and model fitting (`parsnip`).

## Get the recipe/preprocessor

```{r recipe}
recipe_g <- workflows::extract_preprocessor(fit_rf$workflow_g)

# See the recipe
print(recipe_g)
# See a summary of all steps
summary(recipe)

# See what variables are involved
recipe$var_info

# If you want to see what the recipe does to data:
prepped <- recipes::prep(recipe, training_data)
processed <- recipes::bake(prepped, new_data)
processed |> head()
```

## Get the model specifications
```{r model-specs}
model_spec_g <- workflows::extract_spec_parsnip(fit_rf$workflow_g)
model_spec_m <- workflows::extract_spec_parsnip(fit_rf$workflow_m)

print(model_spec)

# Key properties
model_spec_g$engine    # "ranger" is used in the default random forest
model_spec_g$mode      # "regression" for the outcome model
model_spec_m$mode      # "classification" for the treatment model, since it's binary  
```

### What are the hyperparameters?
```{r hyperparams}
model_spec_g$args
```

# Evaluation metrics

This should all be done on the out of fold predictions. Running `augment()` on your fitted model will give you:
+ The actual values for Y and D
+ The predicted (hat) values for Y and D 
+ The residuals for Y and D (i.e. actual - predicted)

From these consider plotting the following:
+ A scatter plot of actual vs predicted for Y and D
+ A histogram of the residuals for Y and D
+ A QQ plot of the residuals for Y and D

Then consider calculating the following metrics:
+ Mean Squared Error (MSE) for Y and D
+ R-squared for Y and D
+ Correlation between actual and predicted for Y and D
+ Correlation between residuals for Y and D

You can use the `yardstick` package to calculate these metrics. For example:
```{r, eval=FALSE}
library(yardstick)
# Mean Squared Error
mse_y <- mse(data, truth = Y, estimate = .pred_Y)
mse_d <- mse(data, truth = D, estimate = .pred_D)
# R-squared
rsq_y <- rsq(data, truth = Y, estimate = .pred_Y)
rsq_d <- rsq(data, truth = D, estimate = .pred_D)
# Correlation
cor_y <- cor(data$Y, data$.pred_Y)
cor_d <- cor(data$D, data$.pred_D)
```