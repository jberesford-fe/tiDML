---
title: "DoubleML Replication"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DoubleML Replication}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
params:
  run_long: false
---

```{r, include = FALSE}
run_long <- isTRUE(params$run_long)

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  cache = FALSE
)
```
# Replication of DoubleML results with tiDML

I use 401k data to replicate results from <a href="https://arxiv.org/abs/1608.00060">Chernozhukov et al. (2018)</a>. Note that this is the example given in their <a href="https://docs.doubleml.org/stable/examples/py_double_ml_pension.html">package documentation</a>. 

The goal is to show that - over 100 iterations - `tiDML` produces identical results, with less code and a more interpretable output.

This code takes several hours to run. For ease, I have saved the results from 100 iterations (5 cross folds, 2 reps) in `inst/extdata` folder. To regenerate these yourself, clone this repo and set `run_long` to `true` in params.

# Get data and import packages

To import the 401k data, load the DoubleML (and other required packages).  

```{r load-packages, warning = FALSE, message = FALSE}
library(tiDML)
library(dplyr)
library(purrr)
library(tibble)
library(ggplot2)
library(DoubleML)
library(mlr3)
library(mlr3learners)
```
```{r import-data, warning = FALSE, message = FALSE, eval = run_long}
df401k <- DoubleML::fetch_401k(return_type = "data.frame", instrument = FALSE)
```

Define  the outcome, treatment, and control variables following Chernozhukov et al. (2018) exactly. 

```{r define-cols, warning = FALSE, message = FALSE, eval = run_long}
y_col  <- "net_tfa"
d_col  <- "e401"
x_cols <- c("age","inc","educ","fsize","marr","twoearn","db","pira","hown")
```

# `tiDML` approach

Again following Chernozhukov, I use random forest in both stages. Since e401 is a dummy variable, the first stage is classification while the second is regression. 

```{r run-tidml, warning = FALSE, message = FALSE, eval = run_long}
run_tidml <- function(seed, df, y, d, x, trees_grid, n_folds, n_rep) {
  set.seed(seed)
  
  df <- df |> mutate(!!d := as.factor(!!rlang::sym(d)))

  fit_tidml <- dml_rf(
    data = df,
    y = !!rlang::sym(y),
    d = !!rlang::sym(d),
    x = x,
    trees_grid = trees_grid,
    n_folds = n_folds,
    n_rep = n_rep,
  )

  return(tibble(
    method = "tiDML",
    seed = seed,
    theta = unname(fit_tidml$estimate),
    se    = unname(fit_tidml$se)
  ) |>
    mutate(
      lwr = theta - stats::qnorm(0.975) * se,
      upr = theta + stats::qnorm(0.975) * se
    ))
}

```

# DoubleML approach

Next I use the `DoubleML` package to run the model.

```{r run-dml, warning = FALSE, message = FALSE, eval = run_long}
run_dml <- function(seed, df, y, d, x, trees_grid, n_folds, n_rep) {
  set.seed(seed)
  ## DoubleML :: PLR with ranger
  ml_l <- lrn("regr.ranger",
              num.trees = trees_grid, 
              num.threads = 1,
              respect.unordered.factors = "order")
  
  ml_m <- lrn(
    "classif.ranger",
    num.trees = trees_grid,
    num.threads = 1,
    predict_type="prob"
  )

  dml_data <- DoubleMLData$new(
    data = df, 
    y_col = y, 
    d_cols = d, 
    x_cols = x
  )

  dml <- DoubleMLPLR$new(
    dml_data, 
    ml_l = ml_l, 
    ml_m = ml_m,
    n_folds = n_folds,
    n_rep = n_rep,
    score = "partialling out"
  )

  dml$fit()

  ci <- dml$confint(level = 0.95)
  return(tibble(
    method = "DoubleML",
    seed = seed,
    theta = as.numeric(dml$coef),
    se    = as.numeric(dml$se),
    lwr   = as.numeric(ci[1]),
    upr   = as.numeric(ci[2])
  )
  )
}
```

# Run both methods across many replications

This wrapper function runs both methods and returns a combined data frame.

```{r run-rep, warning = FALSE, message = FALSE, eval = run_long}

replications <- 100L

run_both <- function(seed, df, y, d, x, trees_grid = 1200, n_folds = 2L, n_rep = 2L) {
  tidml_row <- run_tidml(seed, df, y, d, x, trees_grid, n_folds, n_rep)
  dml_row   <- run_dml(seed, df, y, d, x, trees_grid, n_folds, n_rep)
  bind_rows(tidml_row, dml_row)
}

seeds <- 401 + 0:(replications-1L)  
res <- map_dfr(
  seeds,
  ~ run_both(.x, df = df401k, y = y_col, d = d_col, x = x_cols)
)
```

If you do not want to run the long code above, you can load the saved results here:
```{r quick-results, warning = FALSE, message = FALSE, eval = !run_long}
res <- tiDML::replication_results()
```


# Method-by-method densities

There is randomness in both methods so the estimates vary across seeds. Below I plot the densities of estimates across the replications to show that they overlap and have very similar means.

```{r plot-results, warning = FALSE, message = FALSE, fig.alt="DML estimates across packages"}
ggplot(res, aes(theta, fill = method)) +
  geom_density(alpha = 0.35) +
  geom_vline(
    data = res %>% group_by(method) %>% summarize(mean_theta = mean(theta)),
    aes(xintercept = mean_theta, color = method),
    linetype = "dashed", size = 1, show.legend = FALSE
  ) +
  labs(
    title = "DML-PLR estimates across seeds",
    x = "Theta", y = "Density", fill = "Method", colour= "Method"
  ) +
  theme_minimal(base_size = 12)

```

# T-Test shows no differences between packages 

There is no statistically significant difference in mean, standard error, or confidence intervals. 

```{r t-test, warning = FALSE, message = FALSE}
is_not_significant <- function(x, group, data, alpha = 0.05) {
  pval <- stats::t.test(stats::reformulate(group, x), data = data)$p.value
  pval >= alpha
}

same_theta <- is_not_significant("theta", "method", res)
same_se    <- is_not_significant("se",    "method", res)
same_lwr    <- is_not_significant("lwr", "method", res)
same_upr    <- is_not_significant("upr", "method", res)
```

Theta's are equal:
```{r t-test-theta, warning = FALSE, message = FALSE}
same_theta
```
Standard error's are equal:
```{r t-test-se, warning = FALSE, message = FALSE}
same_se
```
Upper and lower bound confidence interval's are equal:
```{r t-test-lwr, warning = FALSE, message = FALSE}
same_lwr
same_upr
```
