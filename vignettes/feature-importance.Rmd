---
title: "Extract feature importance"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Extract feature importance}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r knit-options, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
# Storing fitted models

In either the default `dml_XX()` functions, or when running a custom run_dml(), you have the option to store all fitted models (across both folds and reps) by setting `store_models = TRUE`. This allows you to extract feature importance measures from the underlying models. 

Warning: this can use a lot of memory, especially if you are using a large number of folds and reps.

### Saving with `store_models = TRUE`

```{r setup, warnings=FALSE, message=FALSE}
library(tiDML)

df <- diamonds_sample(n=500, seed=123)

fit_rf <- dml_rf(
  data = df,
  y = "price",
  d = "is_rated_ideal",
  x = c("carat", "depth", "table", "x", "y", "z"),
  store_models = TRUE
)
```
# Extracting coefficients (linear models)

For linear models (e.g. Lasso), you can extract regression coefficients directly. Under the default settings (5 cross folds, 1 rep) there are 10 sets of regression coefficients: five for treatment and five for outcome. 

To access the treatment model for, say, the third crossfold, save your lasso regression as `fit <- dml_XX(...)` and then use the `generics` package to extract the coefficients: `generics::tidy(fit$m_fit[[3]])`.

tiDML's get_feature_coefs() function is a wrapper around this which maps across all cross folds to give a single tibble with coefficients for all folds and reps. 

```{r feature-coefs-linear, warnings=FALSE, message=FALSE}
fit_lasso <- dml_enet(
  data = df,
  y = "price",
  d = "is_rated_ideal",
  x = c("carat", "depth", "table", "x", "y", "z"),
  store_models = TRUE,
  mixture = 1
)

get_feature_coefs(fit_lasso)
```

# Extracting feature importance (tree based models)

Feature importance can be only be extracted for tree based models (decision trees, random forests, or gradient boosted trees). The function `get_feature_importance()` extracts feature importance measures from all the fitted models and returns a tidy data frame.

```{r feature-importance-impurity}
feature_importance <- get_feature_importance(fit_rf, model = "outcome")

print(feature_importance)
```

You can average importance across folds and reps, or plot the feature importance as a distribution.

```{r feature-importance-plot, fig.alt="Feature importance plot"}
library(ggplot2)

feature_importance |>
  ggplot(aes(importance, reorder(variable, importance))) +
  geom_violin(fill="#425a7f", colour="#425a7f") +
  labs(x="Distribution of importance across folds", y="Feature")
```

